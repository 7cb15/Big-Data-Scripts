{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Spark\n",
    "\n",
    "In this homework, we are practicing Apache Spark.\n",
    "\n",
    "You are required to turn in this notebook as BDM\\_HW4\\_Spark\\_**NetId**.ipynb. You will be asked to complete each task using Apache Spark. Output can be printed in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (5 points)\n",
    "\n",
    "You are asked to implement Homework 3 using Spark. The description is provided below for your convenience.\n",
    "\n",
    "You are asked to implement the Social Triangle example discussed in class. In particular, given the email dataset, please list all \"reciprocal\" relationships in the company. Recall that:\n",
    "\n",
    "If A emails B and B emails A, then A and B is *reciprocal*.\n",
    "\n",
    "If A emails B but B doesn’t email A, then A and B is *directed*.\n",
    "\n",
    "**Dataset:** We will use a subset of the open [Enron Email Dataset](https://www.cs.cmu.edu/~./enron/ \"Enron Email Dataset\"), which contains approximately 10,000 simplified email headers from the Enron Corporation. You can download this dataset from NYU Classes as **enron_mails_small.csv**. The file contains 3 columns *Date*, *From*, and *To*. Their description is as follows:\n",
    "\n",
    "|Column name|Description|\n",
    "|--|--|\n",
    "|Date |The date and time of the email, in the format YYYY-MM-DD hh-mm-ss, <br />e.g. \"1998-10-30 07:43:00\" |\n",
    "|From |The sender email address, <br />e.g. \"mark.taylor@enron.com\" |\n",
    "|To | A list of recipients' email addresses separated by semicolons ';', <br />e.g. \"jennifer.fraser@enron.com;jeffrey.hodge@enron.com\" |\n",
    "\n",
    "Note that, we only care about users employed by Enron, or only relationships having email addresses that end with *'@enron.com'*.\n",
    "\n",
    "The expected output is also provided below. For each reciprocal relationship, please output a tuple consisting of two strings. The first one is always **'reciprocal'**. And the second one is a string showing the name of the two person in the following format: **'Jane Doe : John Doe'**. The names should be presented in the lexical order, i.e. there will not be a 'John Doe : Jane Doe' since 'Jane' is ordered before 'John.\n",
    "\n",
    "Though the dataset only contains email addresses, not actual names, we're assuming that the email aliases were created based on their name. For example:\n",
    "\n",
    "|Email Address|Converted Name|\n",
    "|--|--|\n",
    "|mark.taylor@enron.com|Mark Taylor|\n",
    "|alan.aronowitz@enron.com|Alan Aronowitz|\n",
    "|marc.r.cutler@enron.com|Marc R Cutler|\n",
    "|hugh@enron.com|Hugh|\n",
    "\n",
    "Please fill the code block with a series of MapReduce jobs using your own mapper and reducer functions. Be sure to include the naming convention logic into one of your mappers and/or reducers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize session / spark container\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Enron Emails\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bacon.adrf.info:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Enron Emails</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd8bc545320>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test container\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the email file csv \n",
    "enron = 'enron_mails_small.csv'\n",
    "enron_df = sc.read.csv(enron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial filtering\n",
    "enron_df = enron_df.select(\"_c1\",\"_c2\")\n",
    "enron_df = enron_df.select(\"_c1\",explode(split(enron_df[\"_c2\"],\";\")).alias(\"to\"))\n",
    "enron_df = enron_df.filter(\"_c1 like '%@enron.com%'\") #filter out non-enron emails\n",
    "\n",
    "#clean up the dataframe\n",
    "enron_df = enron_df.filter(\"to like '%@enron.com%'\") #filter out non-enron emails\n",
    "enron_df = enron_df.filter(\"_c1 like '%@enron.com%'\") #filter out non-enron emails\n",
    "enron_df = enron_df.withColumn('_c1', regexp_replace('_c1', '@enron.com', '')) #remove email strings\n",
    "enron_df = enron_df.withColumn('to', regexp_replace('to', '@enron.com', '')) #remove email strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove special characters\n",
    "period_remove = udf(lambda x : x.replace('.', ' '))\n",
    "enron_df = enron_df.withColumn('_c1',period_remove(enron_df._c1))\n",
    "enron_df = enron_df.withColumn('to',period_remove(enron_df.to))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create a dataframe with c1 is the from person and c2 is a set of everyone they have sent an email to\n",
    "enron_df_from = enron_df.groupby(\"_c1\").agg(collect_set(\"to\"))\n",
    "\n",
    "#create a dataframe where _c2 is the person receiving the email and the set is all the people they've received from\n",
    "enron_df_to = enron_df.groupby(\"to\").agg(collect_set(\"_c1\"))\n",
    "\n",
    "#join dataframes \n",
    "enron_df = enron_df_from.join(enron_df_to,enron_df_from._c1 == enron_df_to.to)\n",
    "\n",
    "#renaming columns\n",
    "enron_df = enron_df.select(col(\"_c1\").alias(\"from\"), col(\"collect_set(to)\").alias(\"to_list\"),col(\"to\").alias(\"to\"),\n",
    "                          col(\"collect_set(_c1)\").alias(\"from_list\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeCols = udf(lambda x,y: list(set(x) & set(y)),returnType='array<string>')\n",
    "enron_df = enron_df.withColumn(\"to_from_list\", mergeCols(col(\"to_list\"), col(\"from_list\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_df = enron_df.select(col(\"from\"),col(\"to_from_list\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterdf = enron_df.filter(size('to_from_list')>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterdf = filterdf.withColumn(\"to_from_string\", filterdf[\"to_from_list\"].cast('String'))\n",
    "filterdf = filterdf.select(\"from\",explode(split(filterdf[\"to_from_string\"],\",\")).alias(\"to\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "char_remove = udf(lambda x : x.replace('[', ''))\n",
    "filterdf = filterdf.withColumn('to',char_remove(filterdf.to))\n",
    "\n",
    "char_remove = udf(lambda x : x.replace(']', ''))\n",
    "filterdf = filterdf.withColumn('to',char_remove(filterdf.to))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterdf = filterdf.withColumn('from', ltrim(filterdf['from']))\n",
    "filterdf = filterdf.withColumn('to', ltrim(filterdf['to']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_list = udf(lambda x,y: sorted((x+','+y).split(',')),returnType='array<string>')\n",
    "filterdf = filterdf.withColumn(\"sorted_list\", sort_list(col(\"to\"), col(\"from\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filterdf = filterdf.select(filterdf.sorted_list[0],filterdf.sorted_list[1])\n",
    "filterdf = filterdf.select([\"sorted_list[0]\",'sorted_list[1]']).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dup_remove = udf(lambda x,y: 1 if x==y else 0)\n",
    "filterdf = filterdf.withColumn('same_flag',dup_remove(filterdf['sorted_list[0]'],filterdf['sorted_list[1]']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterdf = filterdf.filter(\"same_flag = 0\")\n",
    "filterdf = filterdf.withColumn('reciprocal',concat(col('sorted_list[0]'),lit(\" : \"),col('sorted_list[1]')))\n",
    "filterdf = filterdf.select(['reciprocal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(reciprocal='janette elbertson : richard sanders'),\n",
       " Row(reciprocal='carol clair : richard sanders'),\n",
       " Row(reciprocal='mark taylor : tana jones'),\n",
       " Row(reciprocal='mark haedicke : richard sanders'),\n",
       " Row(reciprocal='carol clair : sara shackleton'),\n",
       " Row(reciprocal='pinnamaneni krishnarao : vince kaminski'),\n",
       " Row(reciprocal='debra perlingiere : kevin ruscitti'),\n",
       " Row(reciprocal='drew fossum : susan scott'),\n",
       " Row(reciprocal='grant masson : vince kaminski'),\n",
       " Row(reciprocal='carol clair : debra perlingiere'),\n",
       " Row(reciprocal='elizabeth sager : mark taylor'),\n",
       " Row(reciprocal='carol clair : mark taylor'),\n",
       " Row(reciprocal='michelle cash : twanda sweet'),\n",
       " Row(reciprocal='gerald nemec : susan scott'),\n",
       " Row(reciprocal='mark haedicke : twanda sweet'),\n",
       " Row(reciprocal='rosalee fleming : steven kean'),\n",
       " Row(reciprocal='elizabeth sager : richard sanders'),\n",
       " Row(reciprocal='brenda whitehead : elizabeth sager'),\n",
       " Row(reciprocal='janette elbertson : mark taylor'),\n",
       " Row(reciprocal='eric bass : susan scott'),\n",
       " Row(reciprocal='stinson gibner : vince kaminski'),\n",
       " Row(reciprocal='elizabeth sager : mark haedicke'),\n",
       " Row(reciprocal='fletcher sturm : greg whalley'),\n",
       " Row(reciprocal='shirley crenshaw : vince kaminski'),\n",
       " Row(reciprocal='elizabeth sager : janette elbertson'),\n",
       " Row(reciprocal='fletcher sturm : sally beck'),\n",
       " Row(reciprocal='mark taylor : sara shackleton'),\n",
       " Row(reciprocal='carol clair : tana jones'),\n",
       " Row(reciprocal='mark haedicke : michelle cash'),\n",
       " Row(reciprocal='greg whalley : richard sanders'),\n",
       " Row(reciprocal='richard sanders : sara shackleton'),\n",
       " Row(reciprocal='liz taylor : mark haedicke'),\n",
       " Row(reciprocal='mark haedicke : mark taylor'),\n",
       " Row(reciprocal='sara shackleton : tana jones'),\n",
       " Row(reciprocal='vasant shanbhogue : vince kaminski')]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterdf.head(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (5 points)\n",
    "\n",
    "You are asked to implement Task 2 of Lab 5. The description is provided below for your convenience.\n",
    "\n",
    "We’ll be using two NYC open data sets: the SAT Results and the NYC High School Directory data sets. Both can be downloaded from the links below, or from online class resources.\n",
    "\n",
    "**Dataset**: *Please note that each school is uniquely identified by an DBN code, which should be found on both data sets.*\n",
    "\n",
    "**SAT_Results.csv**\n",
    "Source: https://nycopendata.socrata.com/Education/SAT-Results/f9bf-2cp4  \n",
    "Description: “The most recent school level results for New York City on the SAT. Results are available at the school level for the graduating seniors of 2012.”\n",
    "\n",
    "**DOE_High_School_Directory_2014-2015.csv**\n",
    "Source: https://data.cityofnewyork.us/Education/DOE-High-School-Directory-2014-2015/n3p6-zve2  \n",
    "Description: “Directory of NYC High Schools.”\n",
    "\n",
    "We would like to know how the Math scores vary across bus lines or subway lines serving the schools. Your task is to compute the average Math scores of all schools along each bus line and subway line. You can find the bus and subway lines serving each school in the High School Dictionary as bus and subway columns.\n",
    "\n",
    "The expected results are two lists:\n",
    "1. A list of key/value pairs: with bus line as keys, and the average Math scores as values.\n",
    "2. A list of key/value pairs: with subway line as keys, and the average Math scores as values.\n",
    "\n",
    "The top ten lines with highest score are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S1115', 612),\n",
       " ('M79', 594),\n",
       " ('Q42', 582),\n",
       " ('M22', 574),\n",
       " ('Bx3', 571),\n",
       " ('B52', 560),\n",
       " ('B63', 557),\n",
       " ('B69', 548),\n",
       " ('B54', 543),\n",
       " ('B25', 541)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
